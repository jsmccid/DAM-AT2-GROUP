---
title: "XGBLinear"
output: html_notebook
---

```{r}
library(tidyverse)
library(caret)
library(forcats)
# library(data.table)
# library(mltools)
library(doParallel)
```


```{r}
og_train_data <- readRDS("./AT2_train_STUDENT.rds")
str(og_train_data)
```

```{r}
set.seed(85)
og_train_data <- og_train_data %>% 
  select(-video_release_date, -zip_code, -item_id) %>% 
  na.omit()

og_train_data$timestamp <- as.numeric(og_train_data$timestamp)


og_train = createDataPartition(y = og_train_data$rating, p = 0.7, list = F)
new_train <- og_train_data[og_train, ]
new_test <- og_train_data[-og_train, ]
```

# Base data

```{r, eval = FALSE}
set.seed(85)



# controller for boost

control_lin <- trainControl(method = "adaptive_cv",
                            number = 10,
                            repeats = 5,
                            search = "random",
                            #summaryFunction = postResample(),
                            allowParallel = TRUE,
                            verboseIter = TRUE,
                            sampling = NULL,
                            adaptive = list(min =4, alpha = 0.05, 
                                        method = "gls", complete = TRUE)) 


  ########
  # create test & train 
  
  train <- new_train
  test <- new_test
  # 
  # train_x <- model.matrix(~. -user_id, train[,-7])
  # train_y <- train$rating
  # 
  # test_x <- model.matrix(~. -user_id , test[, -7])
  # test_y <- test$rating
  

  ######
  # train model
  
  # timer
 
  start <- proc.time()

  lin_mod <- train(rating ~. -user_id,
                           data = train,
                           method = "xgbLinear", 
                           tree_method = "exact",
                           # eval_metric = "RMSE",
                           trControl = control_lin,
                           verbose = TRUE,
                           metric = "RMSE",
                           maximize = FALSE,
                           tuneLength= 75)

  end <- proc.time() - start
  end_time <- as.numeric((paste(end[3])))
  
  ######
  # evaluate model
  
  # predictions

#  <- cbind(train, pred = predict(lin_mod, train, type = "raw")) 

  # pred <- data.frame(Target = test_y, preds = pred_class, probs = pred_prob$buy)
  
  # save all outputs
  

#  listout <- llist(xmodel, end_time, pred, confusionm, perf, perf_auc, auc)
 
# assign("complex_boost", listout)
  
save(lin_mod, file = "./models/boostv1.Rdata")
  
  # Fitting nrounds = 4, lambda = 0.679, alpha = 8.82e-05, eta = 0.436 on full training set # fail, was maximising rmse
  
# Fitting nrounds = 99, lambda = 0.0018, alpha = 0.826, eta = 2.95 on full training set
  # RMSE 0.9205
```


```{r}
better_train <- readRDS("G:/git/DAM-AT2-GROUP/better_train.rds")
# str(better_train)
```


```{r}

clust <- makePSOCKcluster(1, cores = 5)
registerDoParallel(clust)

set.seed(85)

#transformations

better_train$user_id <- as.numeric(better_train$user_id)
better_train$item_id <- as.numeric(better_train$item_id)

better_train$release_date <- as.numeric(better_train$release_date) 
better_train$timestamp <- as.numeric(better_train$timestamp)

better_train$State <- as.factor(better_train$State)

better_train <- better_train %>% mutate_if(is.factor,
                         fct_explicit_na,
                         na_level = "missing")

better_train$older_than_reviewer <- as.factor(better_train$older_than_reviewer)
better_train$older_than_reviewer <- fct_explicit_na(better_train$older_than_reviewer, na_level = "missing")

better_train_pre <- preProcess(better_train, method = "bagImpute")
better_train <- predict(better_train_pre, newdata = better_train)

better_train_x <- better_train %>% 
  select(-rating)

better_train_y <- better_train$rating

better_train_1h <- dummyVars(~. -user_id, data = better_train_x, fullRank = TRUE)
better_train_1h <- predict(better_train_1h, newdata = better_train_x)



 
  nas <- better_train %>% 
    filter_all(any_vars(is.na(.)))
 # 
 # str(better_train)

stopCluster(clust)

```


```{r}

set.seed(85)




# controller for boost

control_lin <- trainControl(method = "adaptive_cv",
                            number = 10,
                            repeats = 5,
                            search = "random",
                            #summaryFunction = postResample(),
                            allowParallel = TRUE,
                            verboseIter = TRUE,
                            sampling = NULL,
                            adaptive = list(min =4, alpha = 0.05, 
                                        method = "gls", complete = TRUE)) 

  ######
  # train model
  
  # timer
 
  start <- proc.time()

  lin_mod_better <- train(x = better_train_1h, y = better_train_y,
                           # data = better_train_1h,
                           method = "xgbLinear", 
                           tree_method = "exact",
                           # eval_metric = "RMSE",
                           trControl = control_lin,
                           verbose = TRUE,
                           metric = "RMSE",
                           maximize = FALSE,
                           tuneLength = 25,
                           nthread = 14)

  end <- proc.time() - start
  end_time <- as.numeric((paste(end[3])))
  
  ######
  # evaluate model
  
  # predictions

#  <- cbind(train, pred = predict(lin_mod, train, type = "raw")) 

  # pred <- data.frame(Target = test_y, preds = pred_class, probs = pred_prob$buy)
  
  # save all outputs
  

#  listout <- llist(xmodel, end_time, pred, confusionm, perf, perf_auc, auc)
 
# assign("complex_boost", listout)
  
# save(lin_mod_better, file = "./models/boost_betterv2.Rdata")
  
  # Fitting nrounds = 37, lambda = 0.117, alpha = 0.000243, eta = 2.89 on full training set
  # 1.173268e-01  2.429251e-04  37       2.88937029  0.8682525  0.4061920  0.6791751  10  
  
  
  # Fitting nrounds = 96, lambda = 1.31e-05, alpha = 0.895, eta = 2.92 on full training set
  #  1.311019e-05  8.946798e-01  96       2.9175928  0.8654961  0.4102696  0.6759124  50   
  

```

```{r}

set.seed(85)




# controller for boost

control_lin <- trainControl(method = "adaptive_cv",
                            number = 10,
                            repeats = 5,
                            search = "random",
                            #summaryFunction = postResample(),
                            allowParallel = TRUE,
                            verboseIter = TRUE,
                            sampling = NULL,
                            adaptive = list(min =4, alpha = 0.05, 
                                        method = "gls", complete = TRUE)) 

  ######
  # train model
  
  # timer
 
  start <- proc.time()

  lin_mod_dart <- train(x = better_train_1h, y = better_train_y,
                           method = "xgbDART", 
                           eval_metric = "rmse",
                           trControl = control_lin,
                           verbose = TRUE,
                           metric = "RMSE",
                           maximize = FALSE,
                           tuneLength = 20,
                           nthread = 14)

  end <- proc.time() - start
  end_time <- as.numeric((paste(end[3])))
  
  ######
  # evaluate model
  
  # predictions

#  <- cbind(train, pred = predict(lin_mod, train, type = "raw")) 

  # pred <- data.frame(Target = test_y, preds = pred_class, probs = pred_prob$buy)
  
  # save all outputs
  

#  listout <- llist(xmodel, end_time, pred, confusionm, perf, perf_auc, auc)
 
# assign("complex_boost", listout)
  
# save(lin_mod_better, file = "./models/boost_betterv2.Rdata")
  
  # Fitting nrounds = 277, max_depth = 7, eta = 0.459, gamma = 8.91, subsample = 0.963, colsample_bytree = 0.5, rate_drop = 0.138, skip_drop = 0.611, min_child_weight = 7 on full training set  
  

```


```{r}
set.seed(85)




# controller for boost

control_lin <- trainControl(method = "adaptive_cv",
                            number = 10,
                            repeats = 5,
                            search = "random",
                            #summaryFunction = postResample(),
                            allowParallel = TRUE,
                            verboseIter = TRUE,
                            sampling = NULL,
                            adaptive = list(min =4, alpha = 0.05, 
                                        method = "gls", complete = TRUE)) 

  ######
  # train model
  
  # timer
 
  start <- proc.time()

  nn_mod_b <- train(x = better_train_1h, y = better_train_y,
                           method = "mlpKerasDropoutCost",
                           trControl = control_lin,
                           verbose = TRUE,
                           metric = "RMSE",
                           maximize = FALSE,
                           tuneLength = 20,
                           nthread = 14)

  end <- proc.time() - start
  end_time <- as.numeric((paste(end[3])))
```



```{r}

clust <- makePSOCKcluster(1, cores = 5)
registerDoParallel(clust)

better_test <- readRDS("G:/git/DAM-AT2-GROUP/better_test.rds")

better_test$user_id <- as.numeric(better_test$user_id)
better_test$item_id <- as.numeric(better_test$item_id)

better_test$release_date <- as.numeric(better_test$release_date) 
better_test$timestamp <- as.numeric(better_test$timestamp)

better_test$State <- as.factor(better_test$State)

better_test <- better_test %>% mutate_if(is.factor,
                         fct_explicit_na,
                         na_level = "missing")

better_test$older_than_reviewer <- as.factor(better_test$older_than_reviewer)
# better_test$older_than_reviewer <- fct_explicit_na(better_test$older_than_reviewer, na_level = "missing")
levels(better_test$older_than_reviewer) <- c("FALSE", "TRUE", "missing")

better_test_pre <- preProcess(better_test, method = "bagImpute")
better_test <- predict(better_test_pre, newdata = better_test)

# better_train_x <- better_train %>%
#   select(-rating)

# better_train_y <- better_test$rating


better_test_1h <- dummyVars(~. -user_id, data = better_test, fullRank = TRUE)
better_test_1h <- predict(better_test_1h, newdata = better_test)

# dplyr::setdiff(colnames(better_test_1h),colnames(better_train_1h))
# 
# dplyr::setdiff(colnames(better_train_1h),colnames(better_test_1h))

#######################################
 
 col.order <- colnames(better_train_1h)
better_test_1h <- better_test_1h[,col.order]


better_test$predictions <- predict(lin_mod_better, better_test_1h)

better_test$user_item <- paste(better_test$user_id, better_test$item_id, sep="_")
preds <- better_test %>% 
  ungroup() %>% 
  select(rating, user_item)

# preds[preds$rating >= 5, "rating"] <- 5
# preds[preds$rating <= 1, "rating"] <- 1

#pred <- data.frame(Target = test_y, preds = pred_class, probs = pred_prob$buy)


stopCluster(clust)

# str(preds)
write_csv(preds, "./predictions/batgb_predictionsv2.csv")

#v1 xgboost linear, darren first round engineered variables
#2 no concatenating range


```


